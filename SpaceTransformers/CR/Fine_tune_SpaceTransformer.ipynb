{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fine-tune_SpaceTransformer.ipynb","provenance":[{"file_id":"1EGh9bdxq6RqIzbvKuptAWvmIBG2EQJzJ","timestamp":1651094331355},{"file_id":"1PshQtAXyscIWtpVTtpSOUe3Kswz6h7-T","timestamp":1633614278125}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9176b04d57504c88a75b00148068562c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae16029257ab4552a3443e7c83c88138","IPY_MODEL_78e78308faa34e0e83e2c7781c2cd9db","IPY_MODEL_621a4f3b0cda4cfa8eaa5afe22a5c03f"],"layout":"IPY_MODEL_1bbde884d73a4d5c9c46fe601a2ccfee"}},"ae16029257ab4552a3443e7c83c88138":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86520fdb0e8546e480e8ad172aa14494","placeholder":"​","style":"IPY_MODEL_9fafc2351866403e899f8bdfbe98e21f","value":"Downloading: 100%"}},"78e78308faa34e0e83e2c7781c2cd9db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3740992c1af4af6926ba18f690366bd","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ae8d988b517426590876f73b2186e40","value":481}},"621a4f3b0cda4cfa8eaa5afe22a5c03f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06cb246225a1411fbb03a219fc782bc8","placeholder":"​","style":"IPY_MODEL_9c8353756f5942d3b02afaaa02d9a242","value":" 481/481 [00:00&lt;00:00, 5.84kB/s]"}},"1bbde884d73a4d5c9c46fe601a2ccfee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86520fdb0e8546e480e8ad172aa14494":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fafc2351866403e899f8bdfbe98e21f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3740992c1af4af6926ba18f690366bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ae8d988b517426590876f73b2186e40":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"06cb246225a1411fbb03a219fc782bc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c8353756f5942d3b02afaaa02d9a242":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"502c1bee4fde4677aac21bda1e3874be":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a5849c8d30443009ed78a72c03e5b79","IPY_MODEL_b230935c1d3f434a801ca43db3558201","IPY_MODEL_deb101c7db4e4481b08ed3353fa47f72"],"layout":"IPY_MODEL_6bf3fe7b5c39464a9ec1f43e5751c071"}},"2a5849c8d30443009ed78a72c03e5b79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fd0a45329be43ffbb924113022cbd65","placeholder":"​","style":"IPY_MODEL_d49d7f86620f4fd8a7a1c926703c570c","value":"Downloading: 100%"}},"b230935c1d3f434a801ca43db3558201":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f56c709aab534d4baf74f69de3e6257a","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d67b07e54331419bad7ec8dd9b98404b","value":898823}},"deb101c7db4e4481b08ed3353fa47f72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5367144e7bbf4084988792a4c0414050","placeholder":"​","style":"IPY_MODEL_4643fbaa325b460787c75c440f03e7fb","value":" 878k/878k [00:00&lt;00:00, 1.40MB/s]"}},"6bf3fe7b5c39464a9ec1f43e5751c071":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fd0a45329be43ffbb924113022cbd65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d49d7f86620f4fd8a7a1c926703c570c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f56c709aab534d4baf74f69de3e6257a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d67b07e54331419bad7ec8dd9b98404b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5367144e7bbf4084988792a4c0414050":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4643fbaa325b460787c75c440f03e7fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5cb80d0ec5d2421b80204203e77225b9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3cc0127786dd408db9959e59024a0b04","IPY_MODEL_10245e8c529547b691d9b97bb829b6bc","IPY_MODEL_5e33580b85354b17a5c1deaacacccc37"],"layout":"IPY_MODEL_6858cf0d4c8e4b68928938ceb281f73f"}},"3cc0127786dd408db9959e59024a0b04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cab687ea247453683e84c79accacabe","placeholder":"​","style":"IPY_MODEL_b3fa90800f2b4ff3bf841e373e60d4f5","value":"Downloading: 100%"}},"10245e8c529547b691d9b97bb829b6bc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_97e59d787b2e44108e4b55d544689155","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f80dfdeda3c49d6b2a16a17628d5286","value":456318}},"5e33580b85354b17a5c1deaacacccc37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c524b61a3d854f90b5e1b2ec82c0f906","placeholder":"​","style":"IPY_MODEL_c3eb14a63d9a4915adb2554f2a09547b","value":" 446k/446k [00:00&lt;00:00, 1.65MB/s]"}},"6858cf0d4c8e4b68928938ceb281f73f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cab687ea247453683e84c79accacabe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3fa90800f2b4ff3bf841e373e60d4f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97e59d787b2e44108e4b55d544689155":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f80dfdeda3c49d6b2a16a17628d5286":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c524b61a3d854f90b5e1b2ec82c0f906":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3eb14a63d9a4915adb2554f2a09547b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f7083f6122a4fed97a0bd9534414841":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_866105eb60d843c580ae36ae2ba695b8","IPY_MODEL_ea0d8d2417c7475e9cdc8eb3595844d0","IPY_MODEL_458d4eec88c5432ea935fe018075a4ef"],"layout":"IPY_MODEL_ecf44a8be0d74ee9a2fe6c419b8ce4b5"}},"866105eb60d843c580ae36ae2ba695b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2aab0dce8234aaeaeaf280b47ffe99b","placeholder":"​","style":"IPY_MODEL_9d7ff6fa77424a86848212c7eb3d9096","value":"Downloading: 100%"}},"ea0d8d2417c7475e9cdc8eb3595844d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a05c98a19cc4495fbe564158433fe6c1","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9cab3de32ed64593ab921173aea042c2","value":1355863}},"458d4eec88c5432ea935fe018075a4ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82577f81172d4430b5f4395e74590a38","placeholder":"​","style":"IPY_MODEL_455e8b6bb9454a12b52e7de6055afdf7","value":" 1.29M/1.29M [00:00&lt;00:00, 1.04MB/s]"}},"ecf44a8be0d74ee9a2fe6c419b8ce4b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2aab0dce8234aaeaeaf280b47ffe99b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d7ff6fa77424a86848212c7eb3d9096":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a05c98a19cc4495fbe564158433fe6c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cab3de32ed64593ab921173aea042c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82577f81172d4430b5f4395e74590a38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"455e8b6bb9454a12b52e7de6055afdf7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b06139743bf64f15b1d03143df3ea6e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_260a95dbb17e4cb4aafff5f94019aca7","IPY_MODEL_a1efda14441143bb9e21910b55a2a93a","IPY_MODEL_6521f050d36143f09bce24450fbc01c2"],"layout":"IPY_MODEL_14a8d67a06c3418e96b99076edd0564f"}},"260a95dbb17e4cb4aafff5f94019aca7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98e3895b74b9494f8ae2e00bbebf3dcd","placeholder":"​","style":"IPY_MODEL_0a6d397e07f44777bb37ae8041a3a2d4","value":"Downloading: 100%"}},"a1efda14441143bb9e21910b55a2a93a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a881a35855e4fd3ae98e6395bb60e38","max":498845934,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c87cd37e0ac44167aa04d9991ea92c74","value":498845934}},"6521f050d36143f09bce24450fbc01c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4616ddcb7eed4d73ae6aa34f5f6e87c0","placeholder":"​","style":"IPY_MODEL_15f183c21936474d9f0f5d8dabc2d0dc","value":" 476M/476M [00:31&lt;00:00, 36.5MB/s]"}},"14a8d67a06c3418e96b99076edd0564f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98e3895b74b9494f8ae2e00bbebf3dcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a6d397e07f44777bb37ae8041a3a2d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a881a35855e4fd3ae98e6395bb60e38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c87cd37e0ac44167aa04d9991ea92c74":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4616ddcb7eed4d73ae6aa34f5f6e87c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15f183c21936474d9f0f5d8dabc2d0dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"Unra_3rMmKt6"},"source":["## Introduction\n","This notebook was created as a tutorial for the puplication \"Space transformers: language modeling for space systems\". It uses the further pre-trained SpaceTransformer models (SpaceBERT, SpaceSciBERT, SpaceRoBERTa) and fine-tunes them on the Concept Recognition task from the paper. \n","\n","---\n","\n","If you use the models for your experiments please cite: \n","\n","\n","Berquand, A., Darm, P., & Riccardi, A. (2021). Space transformers: language modeling for space systems. IEEE Access, 9, 133111-133122. https://doi.org/10.1109/ACCESS.2021.3115659\n"]},{"cell_type":"code","metadata":{"id":"y8whZEfCn_nf"},"source":["# @title Licensed under the MIT License\n","\n","# Copyright (c) 2021 Paul Darm\n","\n","# Permission is hereby granted, free of charge, to any person obtaining a copy\n","# of this software and associated documentation files (the \"Software\"), to deal\n","# in the Software without restriction, including without limitation the rights\n","# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","# copies of the Software, and to permit persons to whom the Software is\n","# furnished to do so, subject to the following conditions:\n","\n","# The above copyright notice and this permission notice shall be included in\n","# all copies or substantial portions of the Software.\n","\n","# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n","# THE SOFTWARE."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c9gaVnIPadbo"},"source":["Google Colab offers free GPUs and TPUs. Since we'll be training a large neural network it's best to take advantage of this (in this case we'll attach a GPU), otherwise training will take a very long time.\n","\n","A GPU can be added by going to the menu and selecting:\n","\n","`Edit 🡒 Notebook Settings 🡒 Hardware accelerator 🡒 (GPU)`\n","\n","Then run the following cell to confirm that the GPU is detected."]},{"cell_type":"code","metadata":{"id":"TS2eWCo_PPtu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651142201839,"user_tz":-60,"elapsed":729,"user":{"displayName":"Alba Espinosa","userId":"14708662281191631675"}},"outputId":"8758c4d1-9d39-4373-e639-f6f7ae4b8462"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Apr 28 10:36:40 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   70C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"6-hFEEYiW5O_","executionInfo":{"status":"ok","timestamp":1651089871484,"user_tz":-60,"elapsed":581,"user":{"displayName":"Alba Espinosa","userId":"13840680940231224099"}},"outputId":"6b21fece-4c13-4814-d866-5a7ad5f49de7","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Apr 27 20:04:30 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9cLcVup0N774","executionInfo":{"status":"ok","timestamp":1651142232422,"user_tz":-60,"elapsed":21593,"user":{"displayName":"Alba Espinosa","userId":"14708662281191631675"}},"outputId":"2fed4556-df5d-4752-d36f-72dec0551d09"},"source":["#@title Install necessary libraries and modules when running notebook on Google Colab\n","!pip install transformers\n","\n","from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import ShuffleSplit \n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import recall_score#\n","from sklearn.metrics import precision_score\n","\n","import spacy\n","import json\n","import time\n","import pandas as pd\n","import warnings"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 9.2 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 47.0 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 36.8 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 7.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 50.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"WNYFbdLYFSTh"},"source":["## Load data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":602},"id":"RxZ3HcgZIdiU","executionInfo":{"status":"ok","timestamp":1651142236965,"user_tz":-60,"elapsed":540,"user":{"displayName":"Alba Espinosa","userId":"14708662281191631675"}},"outputId":"a4667cbf-c2e0-400b-f8f6-169fc3be9f18"},"source":["## Load CR dataset from GitHub\n","!wget https://raw.githubusercontent.com/strath-ace/smart-nlp/master/SpaceTransformers/CR/CR_ECSS_dataset.json\n","dataset = pd.read_json('/content/CR_ECSS_dataset.json')\n","dataset"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-28 10:37:16--  https://raw.githubusercontent.com/strath-ace/smart-nlp/master/SpaceTransformers/CR/CR_ECSS_dataset.json\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1645094 (1.6M) [text/plain]\n","Saving to: ‘CR_ECSS_dataset.json’\n","\n","\rCR_ECSS_dataset.jso   0%[                    ]       0  --.-KB/s               \rCR_ECSS_dataset.jso 100%[===================>]   1.57M  --.-KB/s    in 0.008s  \n","\n","2022-04-28 10:37:16 (207 MB/s) - ‘CR_ECSS_dataset.json’ saved [1645094/1645094]\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["       sentence_id         words             labels\n","0                0            It                  O\n","1                0         shall                  O\n","2                0            be                  O\n","3                0  demonstrated                  O\n","4                0          that                  O\n","...            ...           ...                ...\n","31438          874        energy  Space Environment\n","31439          874    deposition  Space Environment\n","31440          874             ,                  O\n","31441          874             e                  O\n","31442          874             .                  O\n","\n","[31443 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-1da01eaf-2c26-470b-9822-0cd3d9106b7a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_id</th>\n","      <th>words</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>It</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>shall</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>be</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>demonstrated</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>that</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31438</th>\n","      <td>874</td>\n","      <td>energy</td>\n","      <td>Space Environment</td>\n","    </tr>\n","    <tr>\n","      <th>31439</th>\n","      <td>874</td>\n","      <td>deposition</td>\n","      <td>Space Environment</td>\n","    </tr>\n","    <tr>\n","      <th>31440</th>\n","      <td>874</td>\n","      <td>,</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>31441</th>\n","      <td>874</td>\n","      <td>e</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>31442</th>\n","      <td>874</td>\n","      <td>.</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31443 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1da01eaf-2c26-470b-9822-0cd3d9106b7a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1da01eaf-2c26-470b-9822-0cd3d9106b7a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1da01eaf-2c26-470b-9822-0cd3d9106b7a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J6qJnZ8oFesF","executionInfo":{"status":"ok","timestamp":1651142242385,"user_tz":-60,"elapsed":218,"user":{"displayName":"Alba Espinosa","userId":"14708662281191631675"}},"outputId":"20658b64-0ba6-40ad-a59a-d1a96d77e978"},"source":["## Unique values of labels (tags) in dataset \n","tag_vals = dataset['labels'].unique() \n","##Dict to transform numbers back into their original tags  'Quality control' --> 8 / 'O' --> 4\n","tag2idx = {}\n","for count,  tag in enumerate(tag_vals):\n","   tag2idx[tag] = count\n","print(tag2idx)\n","## Dict to transform numbers back into their original tags 4 --> 'O', 2 --> 'Space Environment'\n","tag2name={tag2idx[key] : key for key in tag2idx.keys()}\n","\n","## Add additional tag for \"None\" label of Pytorch\n","## --> see https://huggingface.co/transformers/custom_datasets.html#token-classification-with-w-nut-emerging-entities\n","tag2name[-100]= \"None\"\n","print(tag2name)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{'O': 0, 'Cleanliness': 1, 'Materials / EEEs': 2, 'Nonconformity': 3, 'System engineering': 4, 'Quality control': 5, 'Measurement': 6, 'Parameter': 7, 'GN&C': 8, 'Project Scope': 9, 'OBDH': 10, 'Power': 11, 'Structure & Mechanism': 12, 'Thermal': 13, 'Telecom.': 14, 'Space Environment': 15, 'Project Organisation / Documentation': 16, 'Safety / Risk (Control)': 17, 'Propulsion': 18}\n","{0: 'O', 1: 'Cleanliness', 2: 'Materials / EEEs', 3: 'Nonconformity', 4: 'System engineering', 5: 'Quality control', 6: 'Measurement', 7: 'Parameter', 8: 'GN&C', 9: 'Project Scope', 10: 'OBDH', 11: 'Power', 12: 'Structure & Mechanism', 13: 'Thermal', 14: 'Telecom.', 15: 'Space Environment', 16: 'Project Organisation / Documentation', 17: 'Safety / Risk (Control)', 18: 'Propulsion', -100: 'None'}\n"]}]},{"cell_type":"code","source":["tag_vals"],"metadata":{"id":"Z6RjOgc-2c-m","executionInfo":{"status":"ok","timestamp":1651142244931,"user_tz":-60,"elapsed":246,"user":{"displayName":"Alba Espinosa","userId":"14708662281191631675"}},"outputId":"73c20f9d-df9c-447f-a705-6ae224dc97a0","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['O', 'Cleanliness', 'Materials / EEEs', 'Nonconformity',\n","       'System engineering', 'Quality control', 'Measurement',\n","       'Parameter', 'GN&C', 'Project Scope', 'OBDH', 'Power',\n","       'Structure & Mechanism', 'Thermal', 'Telecom.',\n","       'Space Environment', 'Project Organisation / Documentation',\n","       'Safety / Risk (Control)', 'Propulsion'], dtype=object)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"4cZefYNzXw2X"},"source":["## Data pre-processing "]},{"cell_type":"code","metadata":{"id":"zreMLOfkOigd","executionInfo":{"status":"ok","timestamp":1651142248910,"user_tz":-60,"elapsed":292,"user":{"displayName":"Alba Espinosa","userId":"14708662281191631675"}}},"source":["def tokenize_and_align_labels(examples, labels, tokenizer):\n","    \"\"\"\n","    Taken and adapted  from: https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n","    Adjust the list of lables to the word-piece tokenisation of BERT-type models  \n","    [polyethylene] --> [poly, ##eth, ##yle, ##ne]\n","    [2]            --> [2, -100,-100,-100]\n","\n","    \"\"\"\n","    tokenized_inputs = tokenizer([example for example in examples], padding=True, truncation=False,is_split_into_words=True)\n","\n","    word_piece_labels = []\n","    label_all_tokens = True\n","    for i, label in enumerate(labels):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n","            # ignored in the loss function.\n","            # --> see https://huggingface.co/transformers/custom_datasets.html#token-classification-with-w-nut-emerging-entities\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # We set the label for the first token of each word.\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx])\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            else:\n","                label_ids.append(label[word_idx] if label_all_tokens else -100)\n","            previous_word_idx = word_idx\n","\n","        word_piece_labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = word_piece_labels\n","    return tokenized_inputs"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NXg5cOmsRH_O"},"source":["## Tokenisation\n","\n","The single words in the requirements need to be \"tokenised\", before we can use them as input for the model. \n","\n","Tokenisation consists of two steps: \n","\n","\n","\n","1.   Transforming words into single tokens the models vocabulary (BERT models --> word-piece tokenisation, polyethylene --> poly ##eth ##yle ##ne) \n","2.   Transform the tokens into their corresponing IDs of the vocabulary ( poly ##eth ##yle ##ne --> 26572, 11031, 12844, 2638)\n","\n","Therefore, we need to download the respective tokeniser from Huggingface (BERT - for SpaceBERT, SciBERT - for SpaceSciBERT, RoBERTa for SpaceRoBERTa).\n","We do this by instantiating our tokenizer with the AutoTokenizer.from_pretrained .\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":501,"referenced_widgets":["9176b04d57504c88a75b00148068562c","ae16029257ab4552a3443e7c83c88138","78e78308faa34e0e83e2c7781c2cd9db","621a4f3b0cda4cfa8eaa5afe22a5c03f","1bbde884d73a4d5c9c46fe601a2ccfee","86520fdb0e8546e480e8ad172aa14494","9fafc2351866403e899f8bdfbe98e21f","f3740992c1af4af6926ba18f690366bd","2ae8d988b517426590876f73b2186e40","06cb246225a1411fbb03a219fc782bc8","9c8353756f5942d3b02afaaa02d9a242","502c1bee4fde4677aac21bda1e3874be","2a5849c8d30443009ed78a72c03e5b79","b230935c1d3f434a801ca43db3558201","deb101c7db4e4481b08ed3353fa47f72","6bf3fe7b5c39464a9ec1f43e5751c071","2fd0a45329be43ffbb924113022cbd65","d49d7f86620f4fd8a7a1c926703c570c","f56c709aab534d4baf74f69de3e6257a","d67b07e54331419bad7ec8dd9b98404b","5367144e7bbf4084988792a4c0414050","4643fbaa325b460787c75c440f03e7fb","5cb80d0ec5d2421b80204203e77225b9","3cc0127786dd408db9959e59024a0b04","10245e8c529547b691d9b97bb829b6bc","5e33580b85354b17a5c1deaacacccc37","6858cf0d4c8e4b68928938ceb281f73f","0cab687ea247453683e84c79accacabe","b3fa90800f2b4ff3bf841e373e60d4f5","97e59d787b2e44108e4b55d544689155","2f80dfdeda3c49d6b2a16a17628d5286","c524b61a3d854f90b5e1b2ec82c0f906","c3eb14a63d9a4915adb2554f2a09547b","3f7083f6122a4fed97a0bd9534414841","866105eb60d843c580ae36ae2ba695b8","ea0d8d2417c7475e9cdc8eb3595844d0","458d4eec88c5432ea935fe018075a4ef","ecf44a8be0d74ee9a2fe6c419b8ce4b5","d2aab0dce8234aaeaeaf280b47ffe99b","9d7ff6fa77424a86848212c7eb3d9096","a05c98a19cc4495fbe564158433fe6c1","9cab3de32ed64593ab921173aea042c2","82577f81172d4430b5f4395e74590a38","455e8b6bb9454a12b52e7de6055afdf7"]},"id":"zpHfOAIvfyCM","executionInfo":{"status":"ok","timestamp":1651089900653,"user_tz":-60,"elapsed":5410,"user":{"displayName":"Alba Espinosa","userId":"13840680940231224099"}},"outputId":"776a11ae-9446-4ce2-e868-4acd61169965"},"source":["#=======================================\n","#          DATA PREPARATION \n","#=======================================\n","\n","## Different Tokenizers\n","#tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  ##SpaceBERT\n","#tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased') ## SpaceSciBERT\n","tokenizer = AutoTokenizer.from_pretrained('roberta-base',add_prefix_space=True) ## SpaceRoBERTa\n","\n","label_all_tokens = True\n","\n","sentences = [[word for word in dataset[dataset['sentence_id']==i]['words'].values] for i in dataset['sentence_id'].unique()]\n","labels = [[tag2idx[label] for label in dataset[dataset['sentence_id']==i]['labels'].values] for i in dataset['sentence_id'].unique()]\n","\n","encoded_input = tokenize_and_align_labels(sentences, labels, tokenizer)\n","\n","input_ids = encoded_input[\"input_ids\"]\n","\n","attention_masks = encoded_input[\"attention_mask\"]\n","\n","labels = encoded_input['labels']\n","\n","for i in range(0,5):\n","        print(\"No.%d,len:%d\"%(i,len(input_ids[i])))\n","        print(\"texts:%s\"%(\" \".join(i for i in tokenizer.tokenize(tokenizer.decode(input_ids[i])))))\n","        print(\"No.%d,len:%d\"%(i,len(labels[i])))\n","        #Reduced Tags\n","        print(\"lables:%s\"%(\" \".join(tag2name[j] for j in labels[i])))\n","\n","\n","tr_inputs, val_inputs, tr_tags, val_tags,tr_masks, val_masks = train_test_split(input_ids, labels,attention_masks, \n","                                                     random_state=4, test_size=0.213)\n","                                                           \n","\n","\n","tr_inputs = torch.as_tensor(tr_inputs)\n","val_inputs = torch.as_tensor(val_inputs)\n","tr_tags = torch.as_tensor(tr_tags)\n","val_tags = torch.as_tensor(val_tags)\n","tr_masks = torch.as_tensor(tr_masks)\n","val_masks = torch.as_tensor(val_masks)\n","\n","batch_num = 8\n","\n","# Only set token embedding, attention embedding, no segment embedding\n","train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","train_sampler = RandomSampler(train_data)\n","# Drop last can make batch training better for the last one\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=False)\n","\n","valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n","valid_sampler = SequentialSampler(valid_data)\n","valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_num)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9176b04d57504c88a75b00148068562c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"502c1bee4fde4677aac21bda1e3874be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cb80d0ec5d2421b80204203e77225b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f7083f6122a4fed97a0bd9534414841"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["No.0,len:497\n","texts:<s> ĠIt Ġshall Ġbe Ġdemonstrated Ġthat Ġno Ġadditional Ġcontamination Ġis Ġintroduced Ġduring Ġthe Ġhandling Ġprocess . Ġ Ċ Ġ Ġ Ġ Ġ ĠNOTE Ġ1 ĠCont amination Ġcan Ġbe Ġavoided Ġby Ġusing Ġtw eez ers Ġand Ġclean Ġgloves , Ġand Ġensuring Ġthat Ġgloves Ġand Ġchemicals Ġare Ġcompatible Ġ Ċ Ġ Ġ Ġ Ġ ĠNOTE Ġ2 ĠTypically Ġused Ġgloves Ġare Ġof Ġpowder Ġ- Ġfree Ġnylon , Ġnit ri le , Ġlatex , Ġl int Ġ- Ġfree Ġcotton . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","No.0,len:497\n","lables:None O O O O O O O Cleanliness O O O O O O O O O O O O O O O Cleanliness Cleanliness O O O O O O O O O Cleanliness Cleanliness O O O O O O Materials / EEEs O O O O O O O O O O O O O O O Materials / EEEs Materials / EEEs Materials / EEEs Materials / EEEs O Materials / EEEs Materials / EEEs Materials / EEEs O Materials / EEEs O Cleanliness Cleanliness Cleanliness Cleanliness Cleanliness O None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None\n","No.1,len:497\n","texts:<s> ĠPhysical Ġdamage Ġshall Ġbe Ġavoided Ġby Ġpacking Ġthe Ġpoly ethyl ene Ġor Ġpoly prop ylene Ġwrapped Ġwork pieces Ġin Ġclean , Ġdust - Ġand Ġl int Ġfree Ġmaterial . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","No.1,len:497\n","lables:None Nonconformity Nonconformity O O O O O O Materials / EEEs Materials / EEEs Materials / EEEs O Materials / EEEs Materials / EEEs Materials / EEEs O System engineering System engineering O Cleanliness O Cleanliness Cleanliness O Cleanliness Cleanliness Cleanliness O O None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None\n","No.2,len:497\n","texts:<s> ĠSam ples Ġshall Ġonly Ġbe Ġhandled Ġwith Ġclean Ġnylon Ġor Ġl int Ġfree Ġgloves . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","No.2,len:497\n","lables:None Quality control Quality control O O O O O O Materials / EEEs O Cleanliness Cleanliness Cleanliness Cleanliness O None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None\n","No.3,len:497\n","texts:<s> ĠThe Ġsupplier Ġshall Ġavoid Ġphysical Ġdamage Ġby Ġpacking Ġthe Ġpoly ethyl ene Ġor Ġpoly prop ylene Ġ- Ġwrapped Ġwork pieces Ġin Ġclean , Ġdust - Ġand Ġl int Ġ- Ġfree Ġmaterial . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","No.3,len:497\n","lables:None O O O O Nonconformity Nonconformity O O O Materials / EEEs Materials / EEEs Materials / EEEs O Materials / EEEs Materials / EEEs Materials / EEEs O O O O O O O O O O Cleanliness Cleanliness Cleanliness Cleanliness Cleanliness O None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None\n","No.4,len:497\n","texts:<s> ĠThe Ġsupplier Ġshall Ġremove Ġany Ġparticles Ġor Ġcontamination Ġvisible Ġon Ġthe Ġouter Ġinsulation Ġunder Ġa Ġmagnification Ġof ĠÃĹ 10 Ġwith Ġa Ġclean , Ġl int Ġ- Ġfree Ġcloth . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","No.4,len:497\n","lables:None O O O O O Cleanliness O Cleanliness O O O Materials / EEEs Materials / EEEs O O O O Measurement Measurement O O O O Cleanliness Cleanliness Cleanliness Cleanliness Cleanliness O None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AqUZXt74Swb0","executionInfo":{"status":"ok","timestamp":1651089900655,"user_tz":-60,"elapsed":21,"user":{"displayName":"Alba Espinosa","userId":"13840680940231224099"}},"outputId":"2590baac-3e5d-4d8d-86d0-dc068b4b9e86"},"source":["tokenizer('polyethylene')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [0, 11424, 30298, 2552, 2], 'attention_mask': [1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"cnR_nuGFViPS"},"source":["## Model Loading \n","After data preparation we load a pre-trained SpaceTransformer, for fine-tuning it on CR. Since our tasks is about classifying single words (tokens), we use the AutoModelForTokenClassification class. \n","\n","Like with the tokenizer, the from_pretrained method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which we can get from our tag2idx dictionary).\n","\n","When loading the model, a warning is telling us we are throwing away some weights (the vocab_transform and vocab_layer_norm layers) and randomly initializing some other (the token classifier layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"On-fsj20oMK4","executionInfo":{"status":"ok","timestamp":1651089921661,"user_tz":-60,"elapsed":534,"user":{"displayName":"Alba Espinosa","userId":"13840680940231224099"}},"outputId":"b29c35e2-8115-4d2b-e869-038d9ed6d76f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Apr 27 20:05:21 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":153,"referenced_widgets":["b06139743bf64f15b1d03143df3ea6e6","260a95dbb17e4cb4aafff5f94019aca7","a1efda14441143bb9e21910b55a2a93a","6521f050d36143f09bce24450fbc01c2","14a8d67a06c3418e96b99076edd0564f","98e3895b74b9494f8ae2e00bbebf3dcd","0a6d397e07f44777bb37ae8041a3a2d4","9a881a35855e4fd3ae98e6395bb60e38","c87cd37e0ac44167aa04d9991ea92c74","4616ddcb7eed4d73ae6aa34f5f6e87c0","15f183c21936474d9f0f5d8dabc2d0dc"]},"id":"eRs3qd5DVSRZ","executionInfo":{"status":"ok","timestamp":1651090027129,"user_tz":-60,"elapsed":101288,"user":{"displayName":"Alba Espinosa","userId":"13840680940231224099"}},"outputId":"1bf59f31-21e4-483e-afa9-fa74a94e8cf1"},"source":["## Load pre-trained SpaceBERT from Huggingface for fine-tuning (exchange for any model you want to finetune)\n","\n","model = AutoModelForTokenClassification.from_pretrained(\"icelab/spaceroberta\", num_labels=len(tag2idx))\n","\n","## Train with GPU, if available\n","n_gpu = torch.cuda.device_count()\n","if torch.cuda.is_available():\n","    model.cuda()\n","    if n_gpu >1:\n","        model = torch.nn.DataParallel(model)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/476M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b06139743bf64f15b1d03143df3ea6e6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at icelab/spaceroberta were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at icelab/spaceroberta and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["device"],"metadata":{"id":"u3zXR9ghoPaN","executionInfo":{"status":"ok","timestamp":1651090040738,"user_tz":-60,"elapsed":201,"user":{"displayName":"Alba Espinosa","userId":"13840680940231224099"}},"outputId":"66df1dee-f07a-4b8c-9faf-c2d36d4a2fe9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"ehhcAk-YoQQE","executionInfo":{"status":"ok","timestamp":1651090043267,"user_tz":-60,"elapsed":484,"user":{"displayName":"Alba Espinosa","userId":"13840680940231224099"}},"outputId":"efb5edeb-b4e5-4e85-a7a4-aba398d0ab1b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Apr 27 20:07:22 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    58W / 149W |   1091MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["param_size = 0\n","for param in model.parameters():\n","    param_size += param.nelement() * param.element_size()\n","buffer_size = 0\n","for buffer in model.buffers():\n","    buffer_size += buffer.nelement() * buffer.element_size()\n","\n","size_all_mb = (param_size + buffer_size) / 1024**2\n","print('model size: {:.3f}MB'.format(size_all_mb))"],"metadata":{"id":"LGqQjt1iot2L","executionInfo":{"status":"ok","timestamp":1651090096547,"user_tz":-60,"elapsed":212,"user":{"displayName":"Alba Espinosa","userId":"13840680940231224099"}},"outputId":"f7950bb6-40d1-49ce-c1c8-290f35b713eb","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["model size: 473.296MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"CZx6eoQwX5XD"},"source":["## Train script \n","\n","This training loop is heavily influenced by Chris McCormicks notebook \"BERT Fine-Tuning Sentence Classification\". \n","\n","https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=GI0iOY8zvZzL\n","\n","**From the notebook:**\n","\n","\"> *Thank you to [Stas Bekman](https://ca.linkedin.com/in/stasbekman) for contributing the insights and code for using validation loss to detect over-fitting!*\n","\n","**Training:**\n","- Unpack our data inputs and labels\n","- Load data onto the GPU for acceleration\n","- Clear out the gradients calculated in the previous pass. \n","    - In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n","- Forward pass (feed input data through the network)\n","- Backward pass (backpropagation)\n","- Tell the network to update parameters with optimizer.step()\n","- Track variables for monitoring progress\n","\n","**Evalution:**\n","- Unpack our data inputs and labels\n","- Load data onto the GPU for acceleration\n","- Forward pass (feed input data through the network)\n","- Compute loss on our validation data and track variables for monitoring progress\" \n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KgGq0LDWNhsv","executionInfo":{"status":"ok","timestamp":1651090643505,"user_tz":-60,"elapsed":532886,"user":{"displayName":"Alba Espinosa","userId":"13840680940231224099"}},"outputId":"efdd128c-b822-49e8-a1c1-45742cc23c5f"},"source":["import time\n","import datetime\n","import random\n","import numpy as np\n","from transformers import set_seed\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","#\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","set_seed(seed_val)\n","#=======================================\n","#          Train parameters\n","#=======================================\n","\n","## determines if all layers are fine_tuned or just the last, newly initialized ones \n","FULL_FINETUNING = True\n","\n","\n","if FULL_FINETUNING:\n","    # Fine-tune model all layer parameters\n","    param_optimizer = list(model.named_parameters())\n","    #no_decay = ['bias', 'gamma', 'beta']\n","    no_decay = []\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.00, 'correct_bias' : False},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.0,'correct_bias' : False}\n","    ] \n","else:\n","    # Only fine tune classifier parameters\n","    param_optimizer = list(model.classifier.named_parameters()) \n","    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5) ## lr = learning rate of the model: default value 5e-5\n","\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","epochs = 4\n","\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","train_loss = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        outputs = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","        loss, logits =  outputs[:2]\n","                           \n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","    train_loss.append(avg_train_loss)\n","\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    # total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    #nb_eval_steps = 0\n","\n","    y_true = []\n","    y_pred = []\n","\n","    # Evaluate data for one epoch\n","    for batch in valid_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            (loss, logits) = model(b_input_ids, \n","                                   token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels)[:2]\n","            #loss, logits =  outputs[:2]\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","         # Only predict the real word, mark=0, will not calculate\n","        input_mask = b_input_mask.to('cpu').numpy()\n","\n","        # Transform logits into Labels and compare them for each sentence\n","        for i,mask in enumerate(input_mask):\n","          # Real one\n","          temp_1 = []\n","          # Predict one\n","          temp_2 = []\n","          ## check Attention mask\n","          for j, m in enumerate(mask):\n","            #  print(j)\n","            #  print(m)\n","          # Mark=0, meaning its a pad word, dont compare\n","              if m:\n","                  ### Checking [CLS] and [SEP] Tokens\n","                  if tag2name[label_ids[i][j]] != \"None\" :\n","                      temp_1.append(tag2name[label_ids[i][j]])\n","                      temp_2.append(tag2name[logits[i][j]])\n","              else:\n","                  break\n","        \n","            \n","          y_true.append(temp_1)\n","          y_pred.append(temp_2)\n","\n","        \n","        \n","    y_true_words = [word if word!='O' else word for require in y_true for word in require ]\n","    y_pred_words = [word if word!='O' else word for require in y_pred for word in require ]\n","    \n","    scores = precision_recall_fscore_support(y_true_words,y_pred_words,labels = [label for label in set(y_true_words)if label!='O'])[2:]\n","    ## result dicts \n","    f1_scores = {}\n","    examples = {}\n","    ## add values for each Label\n","    names =  [label for label in set(y_true_words)if label!='O']\n","    for i, label in enumerate(names):\n","\n","      f1_scores[label] = scores[0][i]\n","      examples[label]= scores[1][i]   \n","    \n","    ## adds averaged scores and summed up examples to result dic\n","    f1_scores['weighted'] = f1_score(y_true_words, y_pred_words, average='weighted',labels =[label for label in set(y_true_words)if label!='O'])\n","    examples['sum']= np.sum([examples[key] for key in examples.keys()])\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(valid_dataloader)\n","\n","    ## print validation loss and F1 score for epoch \n","    \n","    print(\"  F1_score: {0:.2f}\".format(f1_scores['weighted']))\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'F1 score': f1_scores['weighted'],\n","            'examples_sum': examples['sum'],\n","            'Label_F1_scores':f1_scores,\n","            'examples'    : examples,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","    ## save training values to file\n","    with open(\"Train_results.json\", 'w+', encoding='utf-8') as file:\n","                      pd.DataFrame(training_stats).to_json(file, orient='records', force_ascii=False)\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.89\n","  Training epoch took: 0:02:01\n","\n","Running Validation...\n","  F1_score: 0.60\n","  Validation Loss: 0.56\n","  Validation took: 0:00:12\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.38\n","  Training epoch took: 0:02:01\n","\n","Running Validation...\n","  F1_score: 0.65\n","  Validation Loss: 0.49\n","  Validation took: 0:00:12\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.22\n","  Training epoch took: 0:02:02\n","\n","Running Validation...\n","  F1_score: 0.70\n","  Validation Loss: 0.47\n","  Validation took: 0:00:12\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.14\n","  Training epoch took: 0:02:01\n","\n","Running Validation...\n","  F1_score: 0.70\n","  Validation Loss: 0.48\n","  Validation took: 0:00:12\n","\n","Training complete!\n","Total training took 0:08:52 (h:mm:ss)\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"ThQzmKibrMrV","executionInfo":{"status":"ok","timestamp":1651090710462,"user_tz":-60,"elapsed":625,"user":{"displayName":"Alba Espinosa","userId":"13840680940231224099"}},"outputId":"78c998c6-2394-42f6-8a62-2f0ec6f3a415","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Apr 27 20:18:29 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    58W / 149W |   7740MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BVFcm8TaQwdm","executionInfo":{"status":"ok","timestamp":1643280346623,"user_tz":0,"elapsed":655,"user":{"displayName":"paul darm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07529976923683914309"}},"outputId":"3f0475c5-d5d1-4251-9454-7d0a193c2a59"},"source":["## Print classification report with results \n","y_true_words = [word if word!='O' else word for require in y_true for word in require ]\n","y_pred_words = [word if word!='O' else word for require in y_pred for word in require ]\n","report =classification_report(y_true_words, y_pred_words, digits=3, labels = [label for label in set(y_true_words)if label!='O'])\n","print(report)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                      precision    recall  f1-score   support\n","\n","                   Space Environment      0.714     0.717     0.716       293\n","                  System engineering      0.680     0.692     0.686       120\n","                           Parameter      0.529     0.575     0.551       160\n","                     Quality control      0.762     0.749     0.755       231\n","                               Power      0.805     0.841     0.823       182\n","                                GN&C      0.653     0.646     0.649        96\n","                             Thermal      0.667     0.852     0.748       108\n","                         Measurement      0.860     0.908     0.883       217\n","               Structure & Mechanism      0.595     0.566     0.580        83\n","                          Propulsion      0.617     0.430     0.507        86\n","Project Organisation / Documentation      0.377     0.349     0.363        83\n","                       Nonconformity      0.538     0.596     0.566        47\n","             Safety / Risk (Control)      0.857     0.706     0.774        85\n","                       Project Scope      0.632     0.480     0.545        75\n","                    Materials / EEEs      0.727     0.596     0.655        94\n","                            Telecom.      0.512     0.636     0.568        99\n","                         Cleanliness      0.825     0.658     0.732        79\n","                                OBDH      0.800     0.689     0.740       238\n","\n","                           micro avg      0.701     0.688     0.694      2376\n","                           macro avg      0.675     0.649     0.658      2376\n","                        weighted avg      0.704     0.688     0.693      2376\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"dWSZ_J0lWOPT"},"source":["## Save fine-tuned model in drive / to disc"]},{"cell_type":"markdown","metadata":{"id":"zgx2PmCHXp5k"},"source":["#### Connect to drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UCiEKwm9OEM4","executionInfo":{"status":"ok","timestamp":1634722354170,"user_tz":-60,"elapsed":50793,"user":{"displayName":"paul darm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07529976923683914309"}},"outputId":"0c57709f-bca3-4a29-90ae-cec398a8ca8a"},"source":["## connect the notebook to your own Google Drive storage \n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"1DvwotywSQDP"},"source":["#### Save / load model "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CFmwduT0D2c5","executionInfo":{"status":"ok","timestamp":1634726924864,"user_tz":-60,"elapsed":1889,"user":{"displayName":"paul darm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07529976923683914309"}},"outputId":"06ac78a3-b0f9-4499-963d-fb622da5debe"},"source":["## Save Fine-tuned model \n","## Specify path in google drive\n","path = \"/content/drive/My Drive/\"\n","model_out_address = path +'models/Fine-tuned_SpaceBERT'\n","## Save label dicts in model config for loading the model later again \n","model.config.id2label = {key: tag2name[key] for key in tag2name.keys() - {-100}} ## -100 Key need to be deleted from dict otherwise error when loading model again \n","model.config.label2id = tag2idx\n","\n","## Save model\n","model.save_pretrained(model_out_address, save_config = True)\n","## Save tokeniser\n","tokenizer.save_pretrained(model_out_address)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/My Drive/models/Fine-tuned_SpaceBERT/tokenizer_config.json',\n"," '/content/drive/My Drive/models/Fine-tuned_SpaceBERT/special_tokens_map.json',\n"," '/content/drive/My Drive/models/Fine-tuned_SpaceBERT/vocab.txt',\n"," '/content/drive/My Drive/models/Fine-tuned_SpaceBERT/added_tokens.json',\n"," '/content/drive/My Drive/models/Fine-tuned_SpaceBERT/tokenizer.json')"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","metadata":{"id":"oVe4iQuuF13w"},"source":["##Load saved model from drive \n","model = AutoModelForTokenClassification.from_pretrained(model_out_address)#num_labels=len(tag2idx))\n","tokenizer = AutoTokenizer.from_pretrained(model_out_address)\n","## Train with GPU, if available\n","n_gpu = torch.cuda.device_count()\n","if torch.cuda.is_available():\n","    model.cuda();\n","    if n_gpu >1:\n","        model = torch.nn.DataParallel(model)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7pVmsEsw1sp"},"source":["## Or load CR model from Huggingface hub \n","model_file_address = 'icelab/spacescibert_CR'\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"icelab/spacescibert_CR\")\n","\n","model = AutoModelForTokenClassification.from_pretrained(model_file_address)#num_labels=len(tag2idx))\n","n_gpu = torch.cuda.device_count()\n","if torch.cuda.is_available():\n","    model.cuda();\n","    if n_gpu >1:\n","        model = torch.nn.DataParallel(model)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LHvQ7hqpElAl"},"source":["## Example labelling"]},{"cell_type":"code","metadata":{"id":"4ZMhjZIWLj9N"},"source":["text =\"The CubeSat RF design shall either have one RF inhibit and a RF power output no greater than 1.5W at the transmitter antenna's RF input OR the CubeSat shall have a minimum of two independent RF inhibits (CDS 3.3.9) (ISO 5.5.6). \""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hvAl5p_VLbL-"},"source":["import numpy as np\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","#model.eval();\n","sentences = []\n","#spacy.require_gpu()\n","docs = nlp.tokenizer(text)\n","#docs = list(docs)\n","#for sentence in docs:\n","sentences = [word.text for word in docs]\n","\n","encoded_input = tokenizer(sentences, return_tensors=\"pt\", padding=True, is_split_into_words=True)#, truncation=True, max_length=512)\n","input_ids = encoded_input['input_ids']\n","attention_masks = encoded_input[\"attention_mask\"]\n","\n","#b_input_ids = batch[0].to(device)\n","#b_input_mask = batch[1].to(device)\n","i_ids = input_ids.to(device)\n","a_masks = attention_masks.to(device)\n","with torch.no_grad():  \n","  prediction = model(i_ids, token_type_ids=None, attention_mask=a_masks)[0]\n","\n","logits = torch.argmax(F.log_softmax(prediction, dim=2), dim=2)\n","logits = logits.detach().cpu().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Tags from logits without added [CLS] / [SEP] tokens\n","tags_s = [tag2name[t] for t in logits[0][1:-1]]\n","#scores = scores[1:-1]\n","# Count if word was split by tokenizer to split labels of prediction\n","j = 0\n","## Tags, adjusted to wordpiece tokenisation\n","tags_r = []\n","#scores_r = []\n","for word_count, word in enumerate(doc):\n","    ## Tokenise each word of the sentence\n","    word_ids = tokenizer.tokenize(word.text)\n","\n","    ## Tokeniser of Spacy tokenises \"/n\" --> Tokenizer of BERT & RoBERTa doesn't --> len(tokenizer.tokenize(\"\\n\"))= 0 --> Have to tokenise accordingly\n","    if len(word_ids) == 0:\n","      j -= 1\n","      tags_r.append('O')\n","      #scores_r.append(0)\n","      pass\n","    ## == 1 --> Words get not split by word-piece tokeniser\n","    elif len(word_ids) == 1:\n","      # spans.append(mappings[word_count])#s[word_count])\n","      tags_r.append(tags_s[word_count + j])\n","    # scores_r.append(scores[word_count + j])\n","      # pass\n","    ## Word gets split --> Only one prediction gets added to list of tags\n","    else:\n","      tags_r.append(tags_s[word_count + j])\n","    # scores_r.append(scores[word_count + j])\n","      j += (len(word_ids) - 1)\n","\n","\n","#### Get spans in input sentence\n","            \n","## Label of previous word\n","temp_label = ''\n","## word counter, how many steps to go back for complete sequence\n","j = 0\n","for word_count, word in enumerate(doc):\n","    ## Tag is \"0\" and no Label in previous word\n","    if tags_r[word_count] == 'O' and j == 0:\n","        pass\n","    ## Tag is same as before\n","    elif temp_label == tags_r[word_count][2:]:\n","        j -= 1\n","    ## Tag is \"0\" indicating that concept is complete --> add to span and reset word_counter\n","    elif tags_r[word_count] == 'O' and j != 0:\n","        spans.append({\"start\": doc[word_count + j].idx,\n","                      \"end\": (doc[word_count - 1].idx + len(doc[word_count - 1].text)),\n","                      'token_start': doc[word_count + j].i,\n","                      'token_end': doc[word_count - 1].i, \"label\": tags_r[word_count - 1], \n","                  #   'score': np.mean(scores_r[word_count +j:word_count])\n","                      })# [2:]\n","        j = 0  #\n","        temp_label = ''\n","    ## Tag is a Label, word counter starts\n","    else:\n","        temp_label = tags_r[word_count][2:]\n","        j -= 1"],"metadata":{"id":"lp9JhY1vJnkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wVRO232bBHSy"},"source":["def predict_spans(text, model): \n","    \n","    \"\"\" \n","    Function to take a text as input and output span object for use with spacy\n","    text = str\n","    model = Huggingface.Tokenclassificationmodel\n","    \"\"\"\n","\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    model.eval();\n","    sentences = []\n","\n","    docs = nlp.tokenizer(text)\n","\n","    ## use spacy to tokenize the requirement \n","\n","    word_list = [word.text for word in docs]\n","\n","    ## tokenize the word with loaded tokenizer\n","\n","    encoded_input = tokenizer(word_list, return_tensors=\"pt\", padding=True, is_split_into_words=True, truncation=True, max_length=512)\n","    input_ids = encoded_input['input_ids']\n","    attention_masks = encoded_input[\"attention_mask\"]\n","\n","    ## load arrays on device (GPU)\n","    i_ids = input_ids.to(device)\n","    a_masks = attention_masks.to(device)\n","\n","    with torch.no_grad():  \n","      prediction = model(i_ids, token_type_ids=None, attention_mask=a_masks, )[0]\n","\n","    logits = torch.argmax(F.log_softmax(prediction, dim=2), dim=2)\n","    logits = logits.detach().cpu().numpy()\n","\n","    tokens = []\n","    spans = []\n","    \n","    \n","    ## Tags from logits without added [CLS] / [SEP] tokens\n","    tags_s = [tag2name[t] for t in logits[0][1:-1]]\n","    #scores = scores[1:-1]\n","    # Count if word was split by tokenizer to split labels of prediction\n","    j = 0\n","    ## Tags, adjusted to wordpiece tokenisation\n","    tags_r = []\n","    #scores_r = []\n","    for word_count, word in enumerate(doc):\n","        ## Tokenise each word of the sentence\n","        word_ids = tokenizer.tokenize(word.text)\n","\n","        ## Tokeniser of Spacy tokenises \"/n\" --> Tokenizer of BERT & RoBERTa doesn't --> len(tokenizer.tokenize(\"\\n\"))= 0 --> Have to tokenise accordingly\n","        if len(word_ids) == 0:\n","          j -= 1\n","          tags_r.append('O')\n","          #scores_r.append(0)\n","          pass\n","        ## == 1 --> Words get not split by word-piece tokeniser\n","        elif len(word_ids) == 1:\n","          # spans.append(mappings[word_count])#s[word_count])\n","          tags_r.append(tags_s[word_count + j])\n","        # scores_r.append(scores[word_count + j])\n","          # pass\n","        ## Word gets split --> Only one prediction gets added to list of tags\n","        else:\n","          tags_r.append(tags_s[word_count + j])\n","        # scores_r.append(scores[word_count + j])\n","          j += (len(word_ids) - 1)\n","\n","\n","    #### Get spans in input sentence\n","                \n","    ## Label of previous word\n","    temp_label = ''\n","    ## word counter, how many steps to go back for complete sequence\n","    j = 0\n","    for word_count, word in enumerate(doc):\n","        ## Tag is \"0\" and no Label in previous word\n","        if tags_r[word_count] == 'O' and j == 0:\n","            pass\n","        ## Tag is same as before\n","        elif temp_label == tags_r[word_count][2:]:\n","            j -= 1\n","        ## Tag is \"0\" indicating that concept is complete --> add to span and reset word_counter\n","        elif tags_r[word_count] == 'O' and j != 0:\n","            spans.append({\"start\": doc[word_count + j].idx,\n","                          \"end\": (doc[word_count - 1].idx + len(doc[word_count - 1].text)),\n","                          'token_start': doc[word_count + j].i,\n","                          'token_end': doc[word_count - 1].i, \"label\": tags_r[word_count - 1], \n","                      #   'score': np.mean(scores_r[word_count +j:word_count])\n","                          })# [2:]\n","            j = 0  #\n","            temp_label = ''\n","        ## Tag is a Label, word counter starts\n","        else:\n","            temp_label = tags_r[word_count][2:]\n","            j -= 1\n","    return spans"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XlmiJFKr-Fm_","executionInfo":{"status":"ok","timestamp":1643283446336,"user_tz":0,"elapsed":1841,"user":{"displayName":"paul darm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07529976923683914309"}},"outputId":"3e8214ba-c78d-4a83-9bac-cf197b57ef02"},"source":["try:\n","    tag2name\n","except NameError:\n","    tag2name = model.config.id2label\n","    tag2name[-100]=None \n","\n","nlp = spacy.load('en_core_web_sm', disable=['ner'])\n","doc = nlp(text)\n","spans = predict_spans(text, model)\n","concepts =[]\n","for annot in spans:\n","                #concepts.append({'text': data[sentence]['text'],'word': doc.char_span(data[sentence]['spans'][annot]['start'], data[sentence]['spans'][annot]['end'], label=data[sentence]['spans'][annot]['label']).text, 'label':data[sentence]['spans'][annot]['label']})\n","                concepts.append({'word': doc.char_span(annot['start'], annot['end'], label=annot['label']).text, 'label':annot['label']#, 'score':annot['score']\n","                                 })\n","concepts\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': 'System engineering', 'word': 'CubeSat RF design'},\n"," {'label': 'Telecom.', 'word': 'one RF inhibit'},\n"," {'label': 'Telecom.', 'word': 'RF power output'},\n"," {'label': 'Measurement', 'word': '1.5W'},\n"," {'label': 'Telecom.', 'word': 'transmitter antenna'},\n"," {'label': 'Telecom.', 'word': 'RF input'},\n"," {'label': 'System engineering', 'word': 'CubeSat'},\n"," {'label': 'Telecom.', 'word': 'two independent RF inhibits'}]"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"Rsj5OeICD196"},"source":["Visualise with spaCy"]},{"cell_type":"code","metadata":{"id":"6MAaUTwZ-JYl","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1635170555381,"user_tz":-60,"elapsed":191,"user":{"displayName":"paul darm","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07529976923683914309"}},"outputId":"9ccfc502-1a5c-4b4d-d478-e22a2828e37f"},"source":["from spacy.tokens import Span\n","from spacy import displacy\n","## https://spacy.io/usage/visualizers\n","for span in spans:\n","    doc.ents = list(doc.ents) + [Span(doc, span['token_start'], span['token_end']+1, span['label'])]\n","\n","spacy.displacy.render(doc, style='ent', jupyter=True, options={'distance': 90})"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    CubeSat RF design\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Telecom.</span>\n","</mark>\n"," shall either have one \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    RF inhibit\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Telecom.</span>\n","</mark>\n"," and a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    RF power output\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Telecom.</span>\n","</mark>\n"," no greater than \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    1.5W\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Measurement</span>\n","</mark>\n"," at the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    transmitter antenna\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Telecom.</span>\n","</mark>\n","'s \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    RF input\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Telecom.</span>\n","</mark>\n"," OR the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    CubeSat\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">System engineering</span>\n","</mark>\n"," shall have a minimum of two independent \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    RF inhibits\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Telecom.</span>\n","</mark>\n"," (CDS 3.3.9) (ISO 5.5.6). </div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]}]}